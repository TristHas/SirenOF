\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission
\usepackage{neurips_2022}
\usepackage{amsmath}
\usepackage{caption}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2022}
% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2022}
% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{caption}

\title{Optical Flow Regularization of Implicit Neural Representations for Video Frame Interpolation}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  David S.~Hippocampus\thanks{Use footnote for providing further information
    about author (webpage, alternative address)---\emph{not} for acknowledging
    funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}
Recent works have shown the ability of Implicit Neural Representations (INR) to carry meaningful representations of signal derivatives.
In this work, we leverage this property to perform Video Frame Interpolation (VFI)
by explicitly constraining the derivatives of the INR to satisfy the optical flow constraint equation.
We achieve state of the art VFI on limited motion ranges
using only a target video and its optical flow, without learning the interpolation operator from additional training data.
We further show that constraining the INR derivatives not only
allows to interpolate intermediate frames but also improves the ability of narrow networks to fit the observed frames,
which suggests potential applications to video compression and INR optimization.
\end{abstract}

\section{Introduction}

% First paragraph general about the idea of using INR to represent signals. [One paragraph, 4]
%% Many signal processing concepts defined in continuous spaces, but signal representation intrinsically discrete in computers
%% There is a problem with this:
%% In order to apply the infinitesimal quantities defined by theory to discrete sampled representations,
% unrealistic assumptions often have to be explicitly made, and rough heuristics employed.
%% The inability of rough heuristics to hold in generality have favored learning-based approaches

Many core concepts across the fields of signal processing are defined in terms of continuous functions and their derivatives:
surfaces are continuous manifolds in space,
motion is a rate of change in space through time, etc.
In contrast, the modern digital infrastructure is inherently discrete:
digital sensors capture discrete observations of the world sampled in time and space;
computers store and process discrete representations of signals.
In order to model continuous notions on discrete signal representations,
classical works have used different simplifying assumptions,
often taking the form of constant first or second derivatives of the signal between consecutive observations.
The lack of generality of any such handcrafted heuristics,
combined with the ever improving quantitative results of Machine Learning (ML) approaches,
have led to the near ubiquitous use of ML approaches in recent signal processing research.
These approaches leverage large collections of data to infer statistical properties of signals instead of hand-crafted heuristics.
% With the advent of deep learning, recent state-of-the art approaches have moved away from explicitly applying theoretically derived heuristics,
% leveraging the flexibility of end-to-end representations, relying on large training datasets to decide on the heuristics.

% Then a historical approach to frame interpolation and Optical flow.
In computer vision, Video Frame Interpolation (VFI) is one task representative of such development.
VFI models aim to interpolate intermediate frames between the consecutive frames of a video.

To do so, most successful approaches rely on the optical flow
to guide the interpolation of pixel intensities from the pixel grid of observed frames onto the pixel grid of intermediate frames.
Classical approaches formulate assumptions such as constant movement or acceleration fields between consecutive frames [SHO].
The value of each pixel in the inferred intermediate frame is computed by first shifting the pixel intensities of the observed frames following the
optical flow directions, and then interpolating the shifted pixel intensities onto the intermediate frame's pixel grid.
These approaches suffer from the following two main limitations:

\begin{itemize}
\item The optical flow is prone to errors due to obstructions and illumination variations unrelated to the motion field.
\item Assumptions of constant first or second derivatives of the signal do not necessarily hold true in practice.
\end{itemize}

These limitations share a common root cause: discretization.
Indeed, both the optical flow constraint equation, from which is derived the optical flow,
and assumptions of constant motion field only truly hold at the infinitesimal scale,
for time deltas typically much smaller than that of practically used Frames Per Second (FPS).

ML approaches [SHO] have instead proposed to learn the frame interpolation operator from large video collections,
without explicitly formulating any assumption on the signal.
While these approaches have achieved great success in terms of benchmark performance,
they are prone to generalization errors due to domain shifts. % Check me
Indeed differences between the training set distribution (i.e. VFI benchmark videos) and the target video distribution hinders the performance of ML approaches:
differences in the range of motion, exposure time and FPS have been shown to limit the
generalization of state-of-the-art models to video frame interpolation in the wild [SHO]. %check me

% Implicit Neural Representation
%Meanwhile, another line of research has shown that deep architectures themselves provide powerful priors to represent signals (DIP, SIREN).
%Further efforts have been made to understand exactly what priors.
%In recent years, NNs have emerged as powerful parameterization of different signals.
%Why:
%- Depth provides more representational power.
%- Architecture provides inductive bias,
In the mean time, research on implicit representations seeks better discrete representations of continuous signals.
In recent years Implicit Neural Representations (INR), i.e. representing signals as Neural Networks (NN)
have been shown to offer several competitive advantages over explicit representations,
with notable early successes for 3D shape representations \cite{mildenhall2020nerf}.
Of particular interest to us is the work of SIREN \cite{sitzmann2020implicit},
in which the authors have shown that representing
signals using Multi Layer Perceptrons (MLP) with sine activation functions
allowed for meaningful representations of the signal derivatives.
Inspired by this work, we question wether SIREN may be used to guide the interpolation
process of VFI by controlling the exact derivatives of the signal instead of the finite differences
between consecutive discrete frames, thus avoiding the pitfalls of traditional approaches due to discretization.
We do so by constraining the derivatives of SIREN representations to satisfy the optical flow constraint equation,
i.e., to be orthogonal to the video's optical flow
(which we compute using existing state-of-the-art OF models).
We find that this approach outperforms most existing
machine learning-based approaches on small motion range benchmarks,
without relying on machine learning for the interpolation operator.
In this sense, our approaches is most similar to classical VIF approaches,
except that instead of wrapping the OF on discrete explicit frame representations,
we apply the optical flow constraint on the exact gradient of the the INR.
Our method is thus not subject to any mismatch between training and test data.
Furthermore, our approach can sample any number of frame in-between the observed frames
due to the continuous nature of the representation.
In addition to its application to VFI, we also show that constraining the gradient
of the model also improves the ability of narrow MLPs to fit the signal,
suggesting potential applications in INR optimization and video compression.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{Method}
\caption{Illustration of our approach}
\end{figure}

To summarize, the contributions of this work are:
\begin{itemize}
\item We propose a regularization method for SIREN which achieve state-of-the-art video frame interpolation on small motion ranges.
\item In contrast to other state-of-the art approaches, our approach does not rely on training on a large external training set.
It only relies on the target video and its estimated optical flow.
\item We show that our approach not only helps generalizing to intermediate frame,
but also helps narrow models fit the observed frames.
\end{itemize}

On the other hand, our approach (in its current form) presents important limitations:

\begin{itemize}
\item Optimization of the INR is very time-consuming, which hinders our ability to work on full resolution videos for time constraints.
\item Our method currently only works on limited motion range. It does not match state-of-the art ML models on large motion ranges.
\item It relies on an input optical flow, which is computed using existing ML-based model and is thus prone to domain shift generalization errors.
\end{itemize}

While we acknowledge the importance of the above limitations,
we believe these to not be fundamental limitations of our approach but important future INR research directions.
We discuss these limitations at length and present potential ways to approach them in Section \ref{sec_lim}.
The remainder of this paper is organized as follows:
We briefly present some related work in Section \ref{sec_related}, the detail of our method in Section \ref{sec_method},
and design several experiments to highlight the advantages of our approach in Section \ref{sec_exp}.

\section{Related Work}
\label{sec_related}
\textbf{Classical video interpolation.}
The classical approach relies on optical flow to characterize the motion relationship between two frames.
The quality of interpolation depends on the accuracy of the optical flow.
Classical optical flow is usually sparse and can only work at one pixel or sub-pixel level of motion \cite{chen2012real}.
Sparse feature extraction \cite{liu2008sift} is used to enhance the optical flow correctly.
In order to synthesize dense optical flow and work on large displacements, \cite{weinzaepfel2013deepflow} uses sparse convolution and max-pooling.
But instead of learning the model, the parameters are set manually.

\textbf{Deep learning video interpolation.}
A number of deep learning models have been developed for video interpolation tasks.
Almost all models can be categorized as: optical flow based, and kernel based.

\textit{Optical Flow-Based.}
Optical flow-based approaches are the most popular in video frame interpolation.
The standard technique of video frame interpolation aims at explicitly estimating motion in the form of optical flow, warping two input frames to an intermediate frame, and synthesizing the occlusion region. The frames are constrained by the assumption of linear motion and constant luminance between them.
However, video interpolation of video frames is heavily dependent on the accuracy of optical flow.

The Super-SloMo \cite{jiang2018super} proposed by Jiang et,al. is a non-negligible work in the task of optical flow-based video frame interpolation.
Super-SloMo extends the U-Net architecture proposed by Liu et al \cite{liu2017video}.
The bilateral optical flow is calculated for the input two frames and approximates the key frame with the intermediate optical flow of the two frames.
Then the frames of the input are warped according to the obtained intermediate optical flow.

RRIN \cite{li2020video} mentioned that the estimation of intermediate frames in Super-SloMo works poorly near the boundaries because the optical flow is not locally smooth in these regions.
RRIN proposes to improve the accuracy of optical flow by residual learning.
BMBC \cite{park2020bmbc} adds two additional approximate vectors to Super-SloMo to make the bilateral motion estimation more accurate.


\textit{Kernel-Based.}
To avoid explicit motion estimation and warping stages, the kernel-based approach performs a convolution operation on the input frames and the output of the convolution is used as the result of interpolating the frames.
Niklaus et al. \cite{niklaus2017video1} proposed a fully convolutional deep neural network using a spatially adaptive convolutional kernel to perform the prediction of intermediate frames for two frames with consecutive inputs.
Niklaus et al. \cite{niklaus2017video2} improved their method by using a separable convolution with spatially adaptive one-dimensional convolutional kernel pairs estimated for each pixel, in reducing the parameters of the model.
The results of kernel-based methods for frame interpolation can be limited by the size of the kernel.

Lee et al. proposed Adacof \cite{lee2020adacof}, which can use any pixel at any position for convolution operation,
so that the convolution kernel is no longer limited to the local range.
And many methods residing in optical flow are defined as a special case of Adacof.
However, most kernel-based methods can only generate one intermediate frame, and if one wants to generate multiple intermediate frames, one needs to do it recursively.
EDSC \cite{cheng2021multiple} is the first kernel-based method proposed to generate multiple intermediate frames, but the results are not as good as the optical flow method.


\textbf{ Implicit Neural Representation} have encountered early successes in
shape representation and 3D rendering applications [CITE].
Since these early successes, a number of works have attempted to
apply INR to a variety of signals including audio [CITE], images [CITE], videos [CITE],
medical imaging and even climate data \cite{dupont2022coin++}.
A series of recent works have attempted applied INR to video compression [CITE],
with some works \cite{chen2021nerv} even reporting better quality restoration than practical codes on very high compression rates.
These early results are encouraging given the high stakes behind efficient video compression.
Although VFI also finds application in video compression,
we differ from these works as we focus on VFI evaluation of video INR.
Most related to ours is the concurrent work by Shangguan et al. \cite{shangguan2022learning},
which also uses INR for video interpolation tasks.
Their approach, CURE, differs from ours in scope:
they propose to learn a prior on INR for video interpolation,
while we only focus on leveraging INR to guide the interpolation process using a given optical flow.
The work of 

\textbf{Video Frame interpolation}
INR use a neural network to represent an object approximately, which is essentially a way to parameterize the signal.
Since \cite{mildenhall2020nerf}, \cite{sitzmann2020implicit} was developed, INR has performed well in the areas of 3D vision tasks, images, and video.
The image and video tasks most relevant to this paper are around the direction of image/video compression.

COIN \cite{dupont2021coin} first proposed the use of INR to compress images, mapping pixel coordinates to RGB values.
COIN++  cooperated with the meta-learning approach for image compression work based on COIN.
In the field of video compression, NeRV  proposed by Chen et al.
successfully encodes the video into a neural network, i.e., the content of the video is saved using a neural network.
Only the frame index of the model needs to be provided, and the corresponding RGB picture is output.
In other words, this makes it possible to output infinite frames of video using a neural network.
Although NeRV briefly attempts the task of performing video frame interpolation, this is not NeRV's main work.
The NRFF \cite{rho2022neural} proposed by Rho et al., which uses optical flow and residuals information for video compression, does not directly fit all frames.

Most related to our approach
It requires visual features of the video and does not fully map the pixel coordinates and frame positions of the video to RGB images.


\section{Method}
\label{sec_method}
% We fit a video, and test on the middle frames
We consider a ground-truth video as a continuous signal $v$ mapping continuous spatial ($x$, $y$) and temporal ($t$) coordinates to RGB values:

\begin{equation}
\begin{aligned}
v:& \: (x, y, t) \rightarrow (R, G, B) \\
v:& \: \mathbb{R}^3 \rightarrow \mathbb{R}^3
\end{aligned}
\end{equation}

Our goal is to find a continuous function $f_{\theta}$, parameterized by a finite parameter set $\theta \in \Theta$,
with minimum distance $d$ to the ground-truth signal:

\begin{equation}
\begin{aligned}
f_{\theta}:& \:(x, y, t) \rightarrow (R, G, B) \\
s.t. \: \theta =& min_{\Theta} \iiint d(f_{\theta}(x,y,t), v(x,y,t)) dx dy dt
\end{aligned}
\end{equation}

where the distance function $d$ may either be the Peak Signal to Noise Ratio (PSNR) or the Structural Similarity Index Measure (SSIM).
To do so, we only have access to regularly sampled observation of the signal $v$
(i.e. the explicit representation of the video), which we denote as:

\begin{equation}
\begin{aligned}
&\mathcal{V} \in  \: \mathbb{R}^{T \times H \times W \times 3} \\
s.t. \: &\mathcal{V}_{xyt} =   v(x, y, t) %\: , \forall (x, y, t) \in \mathbb{N}^3
\end{aligned}
\end{equation}

% We start by showing that fitting a video does not naturally interpolate.
where $T$ represents the number of frames in the video, and $H \times W$ the spatial resolution.
We use SIREN as parameterized function class $f_{\theta}$.
The most straightforward way to approximate Equation 2 is to optimize the model parameters so as to fit the video frames,
using the following loss function we refer to as the observation loss:

\begin{equation}
\mathcal{L}_{obs} = \frac{1}{HWT} \sum_{x=1}^W\sum_{y=1}^H\sum_{t=1}^T || f_{\theta}(x,y,t) - \mathcal{V}_{xyt} ||^2
\end{equation}

\begin{figure}[t]
\centering
\includegraphics[width=0.8\textwidth]{"w_wo_OF"}
\caption{Illustration of INR frame interpolation with and without optical flow regularization.
Without regularization (middle top), intermediate frames show unnatural high-frequency variations.
Regularizing the INR to satisfy the optical flow constraint equation result in nicely interpolated frames (middle bottom).
}
\label{fig_w_wo_OF}
\end{figure}

However, we found that optimizing the INR to only minimize this observation loss leads to overfitting the observation with high temporal frequencies:
the intra-frame signal, which we aim to correctly recover, shows important deviations from the observed frames, as illustrated in Figure \ref{fig_w_wo_OF}.
This observation has lead us to consider fitting not only the signal itself, but to also constrain its derivatives.
In particular, we regularize the model so as to respect the optical flow constraint.

% But we show that regularizing with the OF does improve the interpolation
%$In computer vision, it is often used as as approximation of the motion field for 3d perception and navigation (cite),
%$or as a useful tool for video compression.
%The optical flow loss we use is defined as follows:

The optical flow constraint equation states that for an infinitesimal lapse of time $\delta t$,
the brightness of physical points perceived by a camera at arbitrary coordinates $(x,y,t)$ should remain constant.
In other words, given the displacement $(\delta x, \delta y)$ of a physical point in the image coordinate system,
the image brightness $v$ should remain constant:
\begin{equation}
v(x, y, t)=v(x + \delta x, y + \delta y, t + \delta t)
\end{equation}

We introduce the vector notation $\textbf{x}=(x,y,t)$ for readability.
Expressing movement as a ratio of displacement in time,
we can write the optical flow $F$ and the above constraint as:

\begin{equation}
\begin{aligned}
F(\textbf{x})=(\frac{\delta x}{\delta t}, \frac{\delta y}{\delta t}, 1) \\
v(\textbf{x})=v(\textbf{x} + F(\textbf{x}))
\end{aligned}
\label{equ_6}
\end{equation}

First order Taylor expansion allows us to rewrite expand equation \ref{equ_6} into the following
\begin{equation}
\begin{aligned}
v(\textbf{x}) = v(\textbf{x}) + \frac{\delta v}{\delta \textbf{x}} \cdot F(\textbf{x}) \\
\frac{\delta v}{\delta \textbf{x}} \cdot F(\textbf{x}) =0
\end{aligned}
\end{equation}


which holds exactly in the limit of infinitesimal $\delta t$.
We leverage this optical flow constraint equation to regularize the INR.
Denoting the derivatives of the SIREN as:

\begin{equation}
D(f, \theta, x, y, t)=\Big(\frac{\delta f_{\theta}(x,y,t)}{\delta x}, \frac{\delta f_{\theta}(x,y,t)}{\delta y}, \frac{\delta f_{\theta}(x,y,t)}{\delta t}\Big)
\end{equation}

we can now define the optical flow regularization loss

\begin{equation}
\mathcal{L}_{of} = \frac{1}{HWT} \sum_{x \in W}\sum_{y \in H}\sum_{t \in T} | D(f, \theta, x, y, t) \cdot F(x, y, t) |
\end{equation}

This loss constrains the derivatives of the signal to be orthogonal to the optical flow and
can be understood as keeping constant brightness along the optical flow trajectories.
%thus dumping the high frequency temporal variations observed in Figure XXX.
The total loss we use to optimize the INR is a weighted sum of these two terms:

\begin{equation}
\mathcal{L} = \lambda \mathcal{L}_{obs} + (1-\lambda) \mathcal{L}_{of}
\end{equation}

where $\lambda$ is a hyper-parameter taking values between 0 and 1, whose impact we investigate in the following section.
The exactness of the optical flow constraint at the infinitesimal scale plays in our favor:
As we regularize the true derivative of the signal representation,
we do not assume constant derivatives of the signal on any time interval.
We believe this to be the main factor behind our positive results.
On the other hand, the optical flow we use was estimated from discrete consecutive frames,
and thus does not represent the true infinitesimal motion field but an estimation of finite differences.
We discuss potential alternatives in Section \ref{sec_lim}.

\section{Experiments}
\label{sec_exp}
Following previous works, we use the Adobe\cite{su2017deep} , X4K\cite{sim2021xvfi} and ND Scene\cite{yoon2020novel} 
datasets as benchmarks to compare our approach to state-of-the-art models.
We use every two frames of the videos as observation frames to fit the SIREN,
and evaluate the ability of to SIREN to interpolate on every other two (intermediate) frames.
For the Adobe dataset, we evaluate our approach on the eight videos that were used as testing set by previous approaches.
We run all additional experiments on the 720p240fps1.mov video of Adobe dataset illustrated in Figure \ref{fig_w_wo_OF}.
Due to the time-consuming operation of optimizing SIREN representations,
we optimize and evaluate all models on a 240$\times$360 resolution,
and we restrict the the Adobe dataset videos to their first 40 frames.
Unless specified otherwise, we use the following default parameters:

SIREN models with depth 9, width 512 and an $\omega$ of 30.
We optimize the models using the Adam optimizer using a cosine learning rate with maximum learning rate of $10^{-5}$ during 5000 epochs.
We uses $\lambda = 0.12$ for the loss function. We compute the optical flow of videos in original resolution using the GMA \cite{jiang2021learning} OF model.

We start by showing the impact of controlling the fit of high frequencies without the optical flow loss in section \ref{sec_of_high}.
We show that limiting the frequency fitted improves interpolation at the cost of degrading the fit of observed frames,
but it does not allow to reach the same accuracy as optical flow regularization,
showing that OF regularization does more than only regularizing the fitted frequency range.
In Section \ref{sec_sota}, we quantitatively compare our results to state of the art models on standard datasets.
We show that our approach achieves state-of-the-art results on videos with low motion ranges,
but underperforms existing methods for videos with high motion ranges.
We present an ablation study in Section \ref{sec_abl},
providing insights and appropriate settings for the different model hyper-parameters,
and a qualitative analysis of our results in Section \ref{sec_qua}.
Finally, we report an additional counter-intuitive result in Section \ref{sec_video_fit}:
We show that our proposed optical flow regularization loss can help the lightweight SIREN model to fit the video better.
This indicates that our method is potentially helpful for video compression.

\subsection{Optical flow constraint and high frequencies}
\label{sec_of_high}
Figure \ref{fig_w_wo_OF} illustrates the fact that applying the optical flow constraint smoothes
out the high-frequency variations from the intermediate frames of vanilla SIREN representations.
Healthy skepticism leads us to start by asking the following question:
Is the impact of the OF constraint limited to smoothing out the undesired high-frequency variations of vanilla SIREN representations?
To do so, we compare the results of vanilla SIREN representations geared towards different frequency ranges,
and quantitatively compare their interpolated frames PSNR to those of OF-constrained representations.
We constrain the SIREN frequency by varying their $\omega$ parameter, and report our comparison in Figure \ref{fig_omega},
with low $\omega$ values corresponding to lower frequency ranges.

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{"omega_wo_of"}
\caption{Evolution of the PSNR of observed and interpolated frames with $\omega$ without OF loss.
Limiting the high frequency fit alone does not reach the same interpolation accuracy as the OF loss.}
\label{fig_omega}
\end{figure}

Constraining the frequency range of vanilla SIREN with $\omega$ down to 5 degrades the fitting of observed frames,
but improves the quality of interpolated frames.
This suggests that $\omega$ behaves similarly to a regularization parameter
controlling a regime of overfitting to the observed frames high frequencies for high $\omega$ values,
versus underfitting for low $\omega$ values.
Figure \ref{fig_omega} further shows that the PSNR of interpolated frames using vanilla SIREN models remain well under the OF-constrained SIREN,
even for the best performing, low-frequency range models,
which confirms than the OF constraint does more than only constraining the fit to high frequencies.
Note that we did not vary the $\omega$ of the OF constrained SIREN in this figure in order to better illustrate our point,
the red line represents results for the best performing $\omega$.
The impact of the $\omega$ parameter on OF-constrained SIREN is illustrated separately in Figure \ref{fig_abl_omega}.

\subsection{State of the art models}
\label{sec_sota}

\begin{table}[!hb]
    \caption{Quantitative comparison to state-of-the-art VFI on Standard benchmarks. Results are formatted as PSNR / SSMI.}
    \begin{minipage}[t]{.5\linewidth}
      \centering
      \caption*{(a) Limited motion range}
      \begin{tabular}[t]{l | l | l }
       &  Adobe-240FPS &  X4K  \\
      \hline
      Super-SloMo \citep{jiang2018super} &  27.77 / 0.886 & 27.38 / 0.852  \\
      RRIN \cite{li2020video}  & 32.37 / 0.962 & 30.70 / 0.927  \\
      BMBC \citep{park2020bmbc}  & 27.83 / 0.917 & 27.42 / 0.858   \\
      AdaCof \cite{lee2020adacof} & 35.50 / 0.968 & 34.61 / 0.921 \\
      ABME   \cite{park2021asymmetric} & 35.28 / 0.966 & 34.30 / 0.919 \\
      FILM   \cite{reda2022film} &	35.97 / 0.971 & \textbf{35.14} / 0.939 \\
      Ours	& \textbf{36.52} / \textbf{0.977} & 35.06/ \textbf{0.944} \\
      \end{tabular}
    \label{tab_a}
    \end{minipage}%
    \begin{minipage}[t]{.5\linewidth}
      \centering
      \caption*{(b) Large motion range}
        \begin{tabular}[t]{l | l }
        	    &   ND Scene  \\
        \hline
        V-NF \cite{mildenhall2020nerf}   &  23.30 / 0.726 \\
        NSFF \cite{li2021neural}   & 28.03 / 0.925 \\
        CURE \cite{shangguan2022learning}   & \textbf{36.91} / \textbf{0.984} \\
        Ours	     & 29.22 / 0.921
        \end{tabular}
	\label{tab_b}
    \end{minipage}

\end{table}

Table \ref{tab_a} quantitatively compares the results of our method to state-of the art VFI models on different datasets.
Despite its simplicity, and without any training data, our approach outperforms most existing models on limited motion ranges (Table \ref{tab_b}).
However, as illustrated in Figure \ref{fig_large}, it falls short of state-of-the-art methods on the more complex ND Scene benchmark due to larger motion ranges.
We provide further comparison in the qualitative analysis of Section \ref{sec_qua} and section \ref{sec_lim}
discusses possible ways forward to bridging the gap performance on large motion datasets.

\subsection{Ablation study}
\label{sec_abl}

Figure \ref{fig_abl} summarizes the impact of the main parameters of our method.
In (a) we observe a trade-off between the observed and interpolated frames quality in the low $\lambda$ ranges.
The quality of interpolated frames peaks at $\lambda=0.12$, beyond which point the interpolated frames quality is limited
by the quality of the fit to the observed frames, in a similar way to the classical overfitting/underfitting trade-off.
However, it should be noted that the PSNR of observed frames for high $\lambda$ differs widely depending on the SIREN's width.
Indeed, as we will show in Section \ref{sec_video_fit}, the OF constraints actually helps narrow SIREN better fit the observed frames.
In (b) and (c) we observe that both higher learning rates and longer training times improve both observed and interpolated frames.
The learning rate is limited in amplitude by instabilities of the optimization procedure,
while the training time is limited by our practical time constraints.
Large $\omega$ (d) also improve the accuracy up to 30, after which instabilities in the training see the accuracy drop abruptly.
Width and depth (e) show interesting co-dependencies:
Increasing width improves interpolation up to a peak after which it degrades.
The peak width gets narrower with increasing depth.

\begin{figure}[h]
\centering
\begin{subfigure}{0.3\textwidth}
	\centering
    \includegraphics[width=1\linewidth]{lambda.jpg}
	\caption{Loss balance $\lambda$}
\end{subfigure}%
\begin{subfigure}{0.27\textwidth}
\centering
    \includegraphics[width=1\linewidth]{LR.jpg}
	\caption{Learning Rate}
\end{subfigure}%
\begin{subfigure}{0.31\textwidth}
	\centering
    \includegraphics[width=1\linewidth]{Epochs.jpg}
    \caption{Epochs}
\end{subfigure}

\begin{subfigure}{0.3\textwidth}
\centering
    \includegraphics[width=1\linewidth]{omega.jpg}
    \caption{$\omega$}
    \label{fig_abl_omega}
\end{subfigure}%
\begin{subfigure}{0.3\textwidth}
\centering

\includegraphics[width=1\linewidth]{Width_Depth.jpg}
    \caption{Width and Depth}
\end{subfigure}
\caption{Impact of our method's main parameters. Plots from (a) to (d)
show both the observed and interpolated frames PSNR
while plot (e) only shows the interpolated frames PSNR}
\label{fig_abl}
\end{figure}


\subsection{Qualitative analysis}
\label{sec_qua}

Figure \ref{fig_small} and \ref{fig_large} provide a qualitative illustration to the results of Section \ref{sec_sota}
The upper frame in Figure \ref{fig_small} shows that our method tends to outperform other methods on videos with limited motion range.
In particular it seems to better catch high spatial frequency regions (grass, sharp edges of the building).
In contrast, large motion as illustrated in Figure \ref{fig_large} shows ghosting effects that the OF regularization is not able to address.

\begin{figure}[h]
\centering
\begin{subfigure}{0.14\textwidth}
	\centering
    \includegraphics[width=1\linewidth]{qua_imgs/720p_240fps_1_ours.jpg}
\end{subfigure}%
\begin{subfigure}{0.14\textwidth}
	\centering
    \includegraphics[width=1\linewidth]{qua_imgs/720p_240fps_1_abme.jpg}
\end{subfigure}%
\begin{subfigure}{0.14\textwidth}
	\centering
    \includegraphics[width=1\linewidth]{qua_imgs/720p_240fps_1_film.jpg}
\end{subfigure}%
\begin{subfigure}{0.14\textwidth}
	\centering
    \includegraphics[width=1\linewidth]{qua_imgs/720p_240fps_1_adacof.jpg}
\end{subfigure}%
\begin{subfigure}{0.14\textwidth}
	\centering
    \includegraphics[width=1\linewidth]{qua_imgs/720p_240fps_1_bmbc.jpg}
    %\caption{Adacof}
\end{subfigure}%
\begin{subfigure}{0.14\textwidth}
	\centering
    \includegraphics[width=1\linewidth]{qua_imgs/720p_240fps_1_rrin.jpg}
    %\caption{Adacof}
\end{subfigure}%
\begin{subfigure}{0.14\textwidth}
	\centering
    \includegraphics[width=1\linewidth]{qua_imgs/720p_240fps_1_super.jpg}
    %\caption{Adacof}
\end{subfigure}
\begin{subfigure}{0.14\textwidth}
	\centering
    \includegraphics[width=1\linewidth]{qua_imgs/TEST02_045_f0465_ours.jpg}
    \caption{Ours}
\end{subfigure}%
\begin{subfigure}{0.14\textwidth}
	\centering
    \includegraphics[width=1\linewidth]{qua_imgs/TEST02_045_f0465_abme.jpg}
    \caption{ABME}
\end{subfigure}%
\begin{subfigure}{0.14\textwidth}
	\centering
    \includegraphics[width=1\linewidth]{qua_imgs/TEST02_045_f0465_film.jpg}
    \caption{FILM}
\end{subfigure}%
\begin{subfigure}{0.14\textwidth}
	\centering
    \includegraphics[width=1\linewidth]{qua_imgs/TEST02_045_f0465_adacof.jpg}
    \caption{AdaCof}
\end{subfigure}%
\begin{subfigure}{0.14\textwidth}
	\centering
    \includegraphics[width=1\linewidth]{qua_imgs/TEST02_045_f0465_bmbc.jpg}
    \caption{BMBC}
\end{subfigure}%
\begin{subfigure}{0.14\textwidth}
	\centering
    \includegraphics[width=1\linewidth]{qua_imgs/TEST02_045_f0465_rrin.jpg}
    \caption{RRIN}
\end{subfigure}%
\begin{subfigure}{0.14\textwidth}
	\centering
    \includegraphics[width=1\linewidth]{qua_imgs/TEST02_045_f0465_super.jpg}
    \caption{SloMo}
\end{subfigure}
\caption{
Small Motion Video Qualitative Analysis.
The interpolated frame results are shown above their residual heat map.
The upper frames show a successfully interpolated frames,
the lower one shows a rare failure case.}
\label{fig_small}
\end{figure}

The lower part of Figure \ref{fig_small} shows a rare failure case of our approach on limited motion ranges:
some artificial stain-like pattern appear in the sky background,
suggesting additional care may be needed especially in low frequency regions.
Despite this rare exception, the overall quality of interpolation
on low motion range videos performs on par with the best existing methods.

\begin{figure}[h]
\centering
\begin{subfigure}{0.2\textwidth}
	\centering
    \includegraphics[width=1\linewidth]{nd_scene/Balloon1_I0.jpg}
    %\caption{$I_0$}
\end{subfigure}%
\begin{subfigure}{0.2\textwidth}
	\centering
    \includegraphics[width=1\linewidth]{nd_scene/Balloon1_out.jpg}
    %\caption{$\hat{I}_0.5$}
\end{subfigure}%
\begin{subfigure}{0.2\textwidth}
	\centering
    \includegraphics[width=1\linewidth]{nd_scene/Balloon1_GT.jpg}
    %\caption{$I_{0.5}$}
\end{subfigure}%
\begin{subfigure}{0.2\textwidth}
	\centering
    \includegraphics[width=1\linewidth]{nd_scene/Balloon1_I1.jpg}
    %\caption{$I_1$}
\end{subfigure}%
\begin{subfigure}{0.2\textwidth}
	\centering
    \includegraphics[width=1\linewidth]{nd_scene/Balloon1_error.png}
    %\caption{Residual Error}
\end{subfigure}
\begin{subfigure}{0.2\textwidth}
	\centering
    \includegraphics[width=1\linewidth]{nd_scene/Skating_I0.jpg}
    \caption{$v(x,y,0)$}
\end{subfigure}%
\begin{subfigure}{0.2\textwidth}
	\centering
    \includegraphics[width=1\linewidth]{nd_scene/Skating_out.jpg}
    \caption{$f_{\theta}(x,y,0.5)$}
\end{subfigure}%
\begin{subfigure}{0.2\textwidth}
	\centering
    \includegraphics[width=1\linewidth]{nd_scene/Skating_GT.jpg}
    \caption{$v(x,y,0.5)$}
\end{subfigure}%
\begin{subfigure}{0.2\textwidth}
	\centering
    \includegraphics[width=1\linewidth]{nd_scene/Skating_I1.jpg}
    \caption{$v(x,y,1)$}
\end{subfigure}%
\begin{subfigure}{0.2\textwidth}
	\centering
    \includegraphics[width=1\linewidth]{nd_scene/Skating_error.png}
    \caption{Residual Error}
\end{subfigure}
\caption{Large Motion Video Qualitative Analysis}
\label{fig_large}
\end{figure}


\subsection{Video fitting}

Figure \ref{fig_video_fit} shows an unexpected side-effect of the OF regularization observed for narrow networks.
As $\mathcal{L}_{obs}$ explicitly maximizes the PSNR of observed frames,
we expected the addition of the $\mathcal{L}_{of}$ term to negatively impact the PSNR of observed frames,
especially for capacity-limited SIREN which should have to compromise between satisfying both loss terms.
It turns out that, for width up to 50, optimizing the SIREN with the additional OF constraint
actually improves the fit to the observed frames.

\label{sec_video_fit}
\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{compression.png}
\caption{Evolution of the \textbf{observed} frames PSNR with depth, with and without OF regularization.
Left: Trend from very narrow to very wide models.
Right: Zoom on the low width regime with the x axis expressed either in number of neurons or corresponding Bits Per Pixel measure.}
\label{fig_video_fit}
\end{figure}

Although a complete investigation of this phenomena is out of the scope of this work,
we highlight how this observation may prove interesting for future work:
From a practical standpoint, improving the fit of low-capacity INR would lead to better video compression INR.
It remains to be seen wether such phenomena may be replicated in more practical architectures such as (i.e. [SHO]).
From a theoretical standpoint, width has been shown to help optimization by linearizing NN Jacobian [SHO].
Thus, understanding how the OF constraint on the NN derivative somehow improves
the optimization landscape of narrow network may lead to interesting insights.

\section{Current limitations and future work}
\label{sec_lim}

While we believe our results to be very promising, the proposed approach in its current is not yet practical for VFI.
Here, we discuss what we believe to be the three main limitations of our approach,
and possible ways to address it.

\textbf{Slow optimization process.} Fitting 20 frames of a video at $240 \times 360$ resolution currently takes 15 hours on a 4$\times$2080Ti GPU using Pytorch.
This computation time is an important drawback as it limits our ability to process full resolution video,
 as well as to explore different hyper parameters and variants of the methods within realistic times.
We expect new methods speeding up the convergence of video INR to be very beneficial to this line of research.
Given recent successes of INR approaches to high impact applications (i.e., video compression [SHO], climate dataset compression [SHO]),
we hopefully expect to see such development in INR optimization research.

% Relying on optical flow model.
\textbf{Reliance on trained optical flow model.}
SIREN models allow us to apply the optical flow on the exact derivatives of the signal,
bypassing the heuristics of classical approach without relying on machine learning.
The optical flow we use, however, is given by a ML model trained on discrete representations, which raises two problems:
it is subject to generalization errors due to domain shifts,
and is subject to finite difference errors such as occlusions.
Future work will aim to bypass our reliance on ML-based OF using alternative constraints on the exact derivatives.

\textbf{Inability to interpolate high motion range videos.}
In its current form, we only apply the optical flow constraint on the observed frames of the video.
This has proven sufficient to reach state-of-the art on low motion ranges but is not sufficient for large motions.
A promising axis of improvement would be to apply further regularization to the interpolates frames (e.g. for intra-frame time indices $t=0.5$)).
Possible regularization methods may include interpolated optical flows or texture similarity constraints as proposed in recent works [COTE],
which may prevent the ghosting effects illustrated in Figure \ref{fig_w_wo_OF}.


\section{Conclusion}

In this paper, we have shown that regularizing INR using the optical flow constraint equation
enables us to perform VFI without relying on ML for the interpolation step.
We have shown that this approach is sufficient to reach state-of-the-art interpolation on videos with low motion range,
without resorting to learning-based interpolation.
This method is not yet practical for high resolution large motion range videos due to the three limitations highlighted in the previous section.
Nevertheless, these limitations can be tackled and we hope the insights presented in this paper can serve prove useful in the future,
either on their own or combined with related approaches.

\bibliographystyle{abbrv}
\bibliography{references}

\end{document}
